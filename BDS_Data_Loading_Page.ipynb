{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic step\n",
    "\n",
    "Goal: multiple input files --> one file\n",
    "\n",
    "==> Read 1 or more files according to the need\n",
    "\n",
    "    file1: (1 files or a directory of files)\n",
    "    file2: (1 files or a directory of files)\n",
    "    file3: (1 files or a directory of files)\n",
    "\n",
    " ==> Combine the files\n",
    "\n",
    "    file1.column1 == file2.column1\n",
    "    file1.column1 == file2.column2\n",
    "\n",
    "\n",
    "## Pandas dataframe - Append/concat\n",
    "\n",
    "bigdata = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "frames = [ process_file(f) for f in dataset_files ]\n",
    "result = pd.append(frames)\n",
    "\n",
    "\n",
    "    frames = [df1, df2, df3]\n",
    "    result = pd.concat(frames)\n",
    "\tor\n",
    "    result = pd.concat(frames, keys=['file1', 'file2', 'file3']) # having keys helps you to trace back\n",
    "\n",
    "\n",
    "    # -- lots of special cases \"Ignoring indexes on the concatenation axis\"\n",
    "\n",
    "\n",
    "## Pandas data frame - JOIN\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "\n",
    "'inner'\n",
    "\n",
    "## Pandas dataframe - Database-style DataFrame joining/merging\n",
    "\n",
    "pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL.\n",
    "\n",
    "#### Python command -- DSL - user has to learn\n",
    "\n",
    "Example - /Users/sampathm/seaborn-data\n",
    "\n",
    "* read\n",
    "* merge\n",
    "\n",
    "# GOAL is to make things simple - not replace or rewrite\n",
    "\n",
    "    dframe1: <main_file> : <files> or [<files1>, <files2>,....]\n",
    "    dframe2: <tags_file> : <files> or [<files1>, <files2>,....]\n",
    "    dframe3: <bills_file> : <files> or [<files1>, <files2>,....]\n",
    "\n",
    "\t>> dframe = read(tag='', data='')\n",
    "\n",
    "###### n files =>> atleast n-1 connections are expected\n",
    "\n",
    "    merge: mdframe1: dict(left=dframe1, right=dframe2, how=inner)\n",
    "    merge: mdframe2: dict(left=dframe1, right=dframe2, how=inner)\n",
    "\n",
    "\t>> mdframe = merge(.., ..)\n",
    "\n",
    "```\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "```\n",
    "\n",
    "Input # local\n",
    "\n",
    "Any files - .csv/.tsv/.avro/.json/.xml -- pandas will do the job\n",
    "\n",
    "* a file\n",
    "* a folder / a list of files\n",
    "* reg-ex path\n",
    "* list of reg-ex paths\n",
    "\n",
    "- HDFS: URL link\n",
    "- DB: Tables\n",
    "- Web Page Data(Pandas can take care of reading it)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To split a file into sub-splits\n",
    "\n",
    "\n",
    "limit = int(df.shape[0] / 5)\n",
    "\n",
    "a, b = 0, limit\n",
    "tmp = df[a:b]\n",
    "tmp.to_csv('data/first.csv')\n",
    "\n",
    "a, b = b, b + limit\n",
    "tmp = df[a:b]\n",
    "tmp.to_html('data/second.html')\n",
    "\n",
    "a, b = b, b + limit\n",
    "tmp = df[a:b]\n",
    "tmp.to_json('data/third.json')\n",
    "\n",
    "a, b = b, b + limit\n",
    "tmp = df[a:b]\n",
    "tmp.to_pickle('data/fourth.pkl')\n",
    "\n",
    "tmp = df[b:]\n",
    "tmp.to_csv('data/rest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Out',\n",
       " '_',\n",
       " '_1',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'concat_data',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'merge_data',\n",
       " 'quit',\n",
       " 'read_data',\n",
       " 'read_utils',\n",
       " 'sample_file',\n",
       " 'sample_folder']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_file = '/Users/sampathm/devbox/infinity/big-data-studio/iris.csv'\n",
    "sample_folder = '/Users/sampathm/devbox/infinity/big-data-studio/data/'\n",
    "\n",
    "\n",
    "from read_utils import *\n",
    "import read_utils\n",
    "\n",
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: ==============================\n",
      "INFO: Reading - /Users/sampathm/devbox/infinity/big-data-studio/data/first.csv\n",
      "INFO: read file - csv/json/...\n",
      "INFO: Reading file(.csv)\t using function -read_csv\n",
      "INFO: ------------------------------\n",
      "INFO: Reading - /Users/sampathm/devbox/infinity/big-data-studio/data/fourth.pkl\n",
      "INFO: read file - csv/json/...\n",
      "INFO: Reading file(.pkl)\t using function -read_pickle\n",
      "INFO: ------------------------------\n",
      "INFO: Reading - /Users/sampathm/devbox/infinity/big-data-studio/data/rest.csv\n",
      "INFO: read file - csv/json/...\n",
      "INFO: Reading file(.csv)\t using function -read_csv\n",
      "INFO: ------------------------------\n",
      "INFO: Reading - /Users/sampathm/devbox/infinity/big-data-studio/data/second.html\n",
      "INFO: read file - csv/json/...\n",
      "INFO: Reading file(.html)\t using function -read_html\n",
      "INFO: ------------------------------\n",
      "INFO: Reading - /Users/sampathm/devbox/infinity/big-data-studio/data/third.json\n",
      "INFO: read file - csv/json/...\n",
      "INFO: Reading file(.json)\t using function -read_json\n",
      "INFO: ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_data(sample_folder)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: read file - csv/json/...\n",
      "INFO: Reading file(.csv)\t using function -read_csv\n",
      "INFO: ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_data(sample_file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# fhandler.setFormatter(formatter)\n",
    "# logger.addHandler(fhandler)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# logging.error('hello!')\n",
    "# logging.debug('This is a debug message')\n",
    "# logging.info('this is an info message')\n",
    "# logging.warning('tbllalfhldfhd, warning.')\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def new_print(prefix, some_input):\n",
    "    print(prefix, some_input)\n",
    "\n",
    "logger.info = lambda x: new_print('INFO:', x)\n",
    "logger.debug = lambda x: new_print('DEBUG:', x)\n",
    "logger.warning = lambda x: new_print('WARNING:', x)\n",
    "logger.warning = lambda x: new_print('WARNING:', x)\n",
    "logger.error = lambda x: new_print('ERROR:', x)\n",
    "\n",
    "import os.path as path\n",
    "\n",
    "\n",
    "sample_file = '/Users/sampathm/devbox/infinity/big-data-studio/iris.csv'\n",
    "sample_folder = '/Users/sampathm/devbox/infinity/big-data-studio/data/'\n",
    "\n",
    "read_functions = {\n",
    "    '.csv' : pd.read_csv,\n",
    "    '.html': pd.read_html,\n",
    "    '.json': pd.read_json,\n",
    "    '.pkl': pd.read_pickle,\n",
    "    '.pickle': pd.read_pickle,\n",
    "}\n",
    "HTML_TABLE = 0rr\n",
    "\n",
    "def read_data(user_input):\n",
    "    if os.path.isfile(user_input):\n",
    "        main_df = read_file(user_input)\n",
    "    elif os.path.isdir(user_input):\n",
    "        main_df = read_dir(user_input)\n",
    "\n",
    "def merge_dataframes():\n",
    "    pass\n",
    "\n",
    "def concat_dataframes():\n",
    "    pass\n",
    "\n",
    "def find_pointer(filename):\n",
    "    for file_extension, file_reader in read_functions.items():\n",
    "        if filename.endswith(file_extension):\n",
    "            logging.info('Reading file(' + file_extension + ')\\t using function -' + str(file_reader.__name__))\n",
    "            return file_reader\n",
    "\n",
    "def read_file(file):\n",
    "    \"\"\"\n",
    "    Returns DF from a file(csv/json/..)\n",
    "    \"\"\"\n",
    "    logger.info('read file - csv/json/...')\n",
    "    file_reader_fns = find_pointer(file)\n",
    "    dataframe = file_reader_fns(file)\n",
    "    if file_reader_fns.__name__ == pd.read_html.__name__:\n",
    "        # read_html returns a list of dataframes in html page\n",
    "        dataframe = dataframe[HTML_TABLE]\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def read_dir(user_input):\n",
    "    \"\"\"\n",
    "    Read a directory\n",
    "    \"\"\"\n",
    "    logger.info('==' * 15)\n",
    "    dir_path, dir_subdirs, all_files = list(os.walk(user_input))[0]\n",
    "    all_dfs = []\n",
    "    for file in all_files:\n",
    "        file = os.path.join(dir_path, file)\n",
    "        logger.info('Reading - ' + file)\n",
    "        try:\n",
    "#             print(locals())\n",
    "            df = read_file(file)\n",
    "#             print(df.head(5))\n",
    "#             logger.info(file + str( df.shape))\n",
    "            all_dfs.append(df)\n",
    "        except:\n",
    "            raise logging.exception('Failed to load file')\n",
    "        logger.info('--' * 15)\n",
    "    if all(map(lambda x: x.__class__  == pd.core.frame.DataFrame, result)):\n",
    "        return pd.concat(all_dfs)\n",
    "    else:\n",
    "        logger.debug('Received a non dataframe object while reading files!')\n",
    "        logger.debug('Printing files list')\n",
    "        logger.debug(str(all_files))\n",
    "        logger.debug('Print types of dataframes received')\n",
    "        logger.debug(str(list(\n",
    "            map(lambda x: x.__class__ , all_dfs)\n",
    "        )))\n",
    "        return all_dfs\n",
    "\n",
    "\n",
    "result = read_dir(sample_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# To check the available functions\n",
    "print([_ for _ in dir(pd) if _.startswith('read')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data source:\n",
    "\n",
    "https://data.worldbank.org/country/india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
